tuneGrid = gbmGrid)
plot(gbm.fit.grid)
plot(gbm.fit, metric = 'kappa')
plot(gbm.fit, metric = 'Kappa')
plot(gbm.fit2, metric = 'Kappa')
gbm.fit <- train(y ~.,
data = train.dat,
# our cv method
trControl = ctrl.rpcv,
method = 'gbm',
preProcess = c('center', 'scale'),
# This will keep gbm from printing a tome
verbose = FALSE,
metric = 'Accuracy')
plot(gbm.fit, metric = 'Kappa')
plot(gbm.fit)
gbm.fit <- train(y ~.,
data = train.dat,
# our cv method
trControl = ctrl.rpcv,
method = 'gbm',
preProcess = c('center', 'scale'),
# This will keep gbm from printing a tome
verbose = FALSE,
metric = 'Accuracy')
plot(gbm.fit)
library(rmarkdown)
knitr::opts_chunk$set(echo = FALSE)
memory.limit(size = 10000000000)
library(`caret`)
library(tidyverse)
library(gridExtra)
set.seed(809)
X <- read.csv("C://Users//Owner//Documents//GitHub//CARET-Presentation-RLadies//X5.csv")
y <- read.csv("C://Users//Owner//Documents//GitHub//CARET-Presentation-RLadies//y5.csv")
y <-
y %>%
mutate(X0 = as.character(X0)) %>%
mutate(y =
case_when(
X0 == '0' ~ 'C',
X0 == '1' ~ 'T',
X0 == '2' ~ 'N')) %>%
select(as.factor(y), -X0)
dat <- cbind.data.frame(X, y)
sample_n(dat, 10)
set.seed(809)
# Data splitting
trainindex <-
createDataPartition(dat$y, # response always goes here
p = 0.75, # fraction to put into training set
list = FALSE)
# trainindex
train.x <- dat[trainindex, -which(names(dat) %in% c('y'))]
train.y <- dat[trainindex, 'y']
test.x <- dat[-trainindex, -which(names(dat) %in% c('y'))]
test.y <- dat[-trainindex, 'y']
train.dat <- dat[trainindex, ]
test.dat <- dat[-trainindex, ]
preproc <- preProcess(dat, c('center', 'scale'))
transformed <- predict(preproc, dat)
data("GermanCredit")
# Plot of non-stratified
n <- nrow(train.x)
samples <- matrix(sample(dat$y, n*4, replace = TRUE), n, 4)
samples.plot <-
samples %>%
as_tibble() %>%
select('A' = 'V1', 'B' = 'V2') %>%
gather() %>%
ggplot(., aes(x = value, fill = key)) +
geom_bar(position = 'dodge')+
ggtitle('Simple Random Sampling') +
theme(legend.position="none") +
xlab("response") +
scale_fill_brewer(palette = 'Set2')
# Plot of stratified
trainIndex <- createDataPartition(dat$y, times = 2, p = 0.75)
samples.caret <- cbind.data.frame(dat$y[trainIndex[[1]]],
dat$y[trainIndex[[2]]])
caret.plot <-
samples.caret %>%
gather() %>%
ggplot(., aes(x = value, fill = key)) +
geom_bar(position = 'dodge') +
ggtitle('Stratified Random Sampling')+
theme(legend.position="none") +
xlab("response")+
scale_fill_brewer(palette = 'Set2')
grid.arrange(samples.plot, caret.plot, ncol = 2)
preproc <- preProcess(train.x, c('center', 'scale'))
preproc
transformed <- predict(preproc, train.x) # This creates a new dataframe
data("Sacramento")
head(Sacramento)
lincombs <- findLinearCombos(model.matrix(price ~., data = Sacramento))
lincombs$remove # recommended columns to remove
data("GermanCredit")
idx <- createDataPartition(GermanCredit$Class, p = 0.75, list = FALSE)
train.GC <- GermanCredit[idx,]
test.GC <- GermanCredit[-idx,]
nearZeroVar(train.GC[ , -which(names(train.GC) %in% c("Class"))])
preProcess(train.GC[ , -which(names(train.GC) %in% c("Class"))], "nzv")
findLinearCombos(model.matrix(Class ~., data = GermanCredit))
# remove variables found from nearZeroVar
GermanCredit2 <-
GermanCredit[, -nearZeroVar(GermanCredit)]
# Remove variables found from findLinearCombos
GermanCredit.ready <-
GermanCredit2 %>%
select(-CheckingAccountStatus.lt.0, -SavingsAccountBonds.lt.100,
-EmploymentDuration.lt.1, -EmploymentDuration.Unemployed,
-Personal.Male.Married.Widowed, -Property.Unknown,
- Housing.ForFree)
# GermanCredit$CheckingAccountStatus.lt.0 <- NULL
# GermanCredit$SavingsAccountBonds.lt.100 <- NULL
# GermanCredit$EmploymentDuration.lt.1 <- NULL
# GermanCredit$EmploymentDuration.Unemployed <- NULL
# GermanCredit$Personal.Male.Married.Widowed <- NULL
# GermanCredit$Property.Unknown <- NULL
# GermanCredit$Housing.ForFree <- NULL
# fits model to training set without resampling
ctrl.none <- trainControl(method = 'none')
# 5-fold cross validation
# ctrl.kcv <- trainControl(method = 'cv', number = 5)
#  10-pete of 10-fold cv
ctrl.rpcv <- trainControl(method = 'repeatedcv', number = 10, repeats = 10)
# ctrl.oob <- trainControl(method = 'oob') # Out of bag sample
create.folds <- createFolds(train.dat$y, k = 10, returnTrain = TRUE)
str(create.folds)
modelLookup('gbm')[, 1:3] # Nice way to extract tuning parameters
train.dat <- train.dat %>% mutate(y = as.factor(y))
set.seed(11)
tmp.index <- createDataPartition(dat$y, p = 0.75, times = 5)
gbm.fit <- train(y ~.,
data = train.dat,
# our cv method
trControl = ctrl.rpcv,
method = 'gbm',
preProcess = c('center', 'scale'),
# This will keep gbm from printing a tome
verbose = FALSE,
metric = 'Accuracy')
gbm.fit
rf.fit <-
train(y ~.,
data = train.dat,
trainControl = 'none', # just one model
method = 'rf',
preProcess = c('center', 'scale'),
metric = 'Accuracy')
rf.fit2 <-
train(y ~.,
data = train.dat,
trainControl = ctrl.kcv,
method = 'rf',
preProcess = c('center', 'scale'),
metric = 'Accuracy')
extractPrediction(gbm.fit, testX = test.dat[, 1:5])
extractPrediction(gbm.fit, testX = test.dat[, 1:5])
?extractPrediction
?createDataPartition
fold.index <- createDataPartition(dat$y, p = 0.80, times = 5)
View(fold.index)
?resamples
rf.compare <-
train(y ~.,
method = 'rf',
trControl = trainControl(method = 'cv', index = fold.index))
rf.compare <-
train(y ~.,
data = dat,
method = 'rf',
trControl = trainControl(method = 'cv', index = fold.index))
gbm.compare <-
train(y ~.,
data = dat,
method = 'gbm',
trControl = trainControl(method = 'cv', index = fold.index))
?createDataPartition
fold.index <- createDataPartition(dat.train$y, p = 0.80, times = 20)
fold.index <- createDataPartition(train.dat$y, p = 0.80, times = 20)
rf.compare <-
train(y ~.,
data = train.dat,
method = 'rf',
trControl = trainControl(method = 'cv', index = fold.index))
gbm.compare <-
train(y ~.,
data = train.dat,
method = 'gbm',
verbose = FALSE,
trControl = trainControl(method = 'cv', index = fold.index))
resamps <- resamples(list(RF = rf.compare,
GBM = gbm.compare))
summary(resamps)
set.seed(110)
fold.index <- createDataPartition(train.dat$y, p = 0.80, times = 100)
rf.compare <-
train(y ~.,
data = train.dat,
method = 'rf',
trControl = trainControl(method = 'cv', index = fold.index))
gbm.compare <-
train(y ~.,
data = train.dat,
method = 'gbm',
verbose = FALSE,
trControl = trainControl(method = 'cv', index = fold.index))
resamps <- resamples(list(RF = rf.compare,
GBM = gbm.compare))
summary(resamps)
?resamples
?trainControl
rf.compare <-
train(y ~.,
data = train.dat,
method = 'rf',
trControl = trainControl(method = 'cv', k = 10, index = fold.index))
rf.compare <-
train(y ~.,
data = train.dat,
method = 'rf',
trControl = trainControl(method = 'cv',
number = 10,
index = fold.index))
gbm.compare <-
train(y ~.,
data = train.dat,
method = 'gbm',
verbose = FALSE,
trControl = trainControl(method = 'cv',
number = 10,
index = fold.index))
resamps <- resamples(list(RF = rf.compare,
GBM = gbm.compare))
summary(resamps)
rf.compare <-
train(y ~.,
data = train.dat,
method = 'rf',
trControl = trainControl(method = 'cv',
number = 5,
index = fold.index))
gbm.compare <-
train(y ~.,
data = train.dat,
method = 'gbm',
verbose = FALSE,
trControl = trainControl(method = 'cv',
number = 5,
index = fold.index))
resamps <- resamples(list(RF = rf.compare,
GBM = gbm.compare))
summary(resamps)
set.seed(110)
fold.index <- createDataPartition(train.dat$y, p = 0.80, times = 100)
rf.compare <-
train(y ~.,
data = train.dat,
method = 'rf',
trControl = trainControl(method = 'repeatedcv',
number = 10,
repeats = 10,
index = fold.index))
gbm.compare <-
train(y ~.,
data = train.dat,
method = 'gbm',
verbose = FALSE,
trControl = trainControl(method = 'repeatedcv',
number = 10,
repeats = 10,
index = fold.index))
# This doesn't actually resample, but contains the results.
resamps <- resamples(list(RF = rf.compare,
GBM = gbm.compare))
GermanCredit.ready
idx <- createDataPartition(GermanCredit.ready$Class, p =0.8)
train.ger <- GermanCredit.ready[idx,]
idx <- createDataPartition(GermanCredit.ready$Class, list = FALSE, p =0.8)
train.ger <- GermanCredit.ready[idx,]
test.ger <- GermanCredit.ready[-idx,]
ger.rf <-train(Class ~.,
data = train.ger,
method = 'gbm',
verbose = FALSE,
trControl = trainControl(method = 'repeatedcv',
number = 10,
repeats = 10))
ger.gbm <-
train(Class ~.,
data = train.ger,
method = 'gbm',
verbose = FALSE,
trControl = trainControl(method = 'repeatedcv',
number = 10,
repeats = 10))
ger.gbm <-
train(Class ~.,
data = train.ger,
method = 'gbm',
verbose = FALSE,
trControl = trainControl(method = 'repeatedcv',
number = 10,
repeats = 10))
ger.rf <-
train(Class ~.,
data = train.ger,
method = 'rf',
verbose = FALSE,
trControl = trainControl(method = 'repeatedcv',
number = 10,
repeats = 10))
plot(ger.gbm)
plot(ger.rf)
plot(ger.gbm)
fold.index <- createFolds(train.dat$y, k = 10)
rf.compare <-
train(y ~.,
data = train.dat,
method = 'rf',
trControl = trainControl(method = 'cv'
index = fold.index))
rf.compare <-
train(y ~.,
data = train.dat,
method = 'rf',
trControl = trainControl(method = 'cv',
index = fold.index))
gbm.compare <-
train(y ~.,
data = train.dat,
method = 'gbm',
verbose = FALSE,
trControl = trainControl(method = 'cv',
index = fold.index))
?createMultiFolds
fold.index <- createFolds(train.dat$y, k = 10, returnTrain = TRUE)
rf.compare <-
train(y ~.,
data = train.dat,
method = 'rf',
trControl = trainControl(method = 'cv',
index = fold.index))
gbm.compare <-
train(y ~.,
data = train.dat,
method = 'gbm',
verbose = FALSE,
trControl = trainControl(method = 'cv',
index = fold.index))
# This doesn't actually resample, but contains the results.
resamps <- resamples(list(RF = rf.compare,
GBM = gbm.compare))
summary(resamps)
dotplot(resamps)
?svm model
?svm
dotplot(resamps)
plot(gbm.fit)
plot(gbm.fit.grid)
gbmGrid <-  expand.grid(# interaction depth from 1-5
interaction.depth = c(1:5),
# n.trees from 20-500 by 20s
n.trees = (1:25)*20,
# shrinkage and n.minobsinnode are constant
shrinkage = 0.01,
n.minobsinnode = 10)
gbm.fit.grid <- train(y~.,
data = train.dat,
trControl = ctrl.rpcv,
method = 'gbm',
preProcess = c('center', 'scale'),
verbose = FALSE,
# Include your grid here
tuneGrid = gbmGrid)
plot(gbm.fit.grid)
modelLookup(model = 'svm')
modelLookup(model = 'svmRadial')
modelLookup(model = 'lm')
modelLookup(model = 'log')
modelLookup(model = 'logir')
modelLookup(model = 'logit')
modelLookup(model = 'pls')
library(rmarkdown)
knitr::opts_chunk$set(echo = FALSE)
memory.limit(size = 10000000000)
library(`caret`)
library(tidyverse)
library(gridExtra)
set.seed(809)
X <- read.csv("C://Users//Owner//Documents//GitHub//CARET-Presentation-RLadies//X5.csv")
y <- read.csv("C://Users//Owner//Documents//GitHub//CARET-Presentation-RLadies//y5.csv")
y <-
y %>%
mutate(X0 = as.character(X0)) %>%
mutate(y =
case_when(
X0 == '0' ~ 'C',
X0 == '1' ~ 'T',
X0 == '2' ~ 'N')) %>%
select(as.factor(y), -X0)
dat <- cbind.data.frame(X, y)
sample_n(dat, 10)
set.seed(809)
# Data splitting
trainindex <-
createDataPartition(dat$y, # response always goes here
p = 0.75, # fraction to put into training set
list = FALSE)
# trainindex
train.x <- dat[trainindex, -which(names(dat) %in% c('y'))]
train.y <- dat[trainindex, 'y']
test.x <- dat[-trainindex, -which(names(dat) %in% c('y'))]
test.y <- dat[-trainindex, 'y']
train.dat <- dat[trainindex, ]
test.dat <- dat[-trainindex, ]
preproc <- preProcess(dat, c('center', 'scale'))
transformed <- predict(preproc, dat)
data("GermanCredit")
# Plot of non-stratified
n <- nrow(train.x)
samples <- matrix(sample(dat$y, n*4, replace = TRUE), n, 4)
samples.plot <-
samples %>%
as_tibble() %>%
select('A' = 'V1', 'B' = 'V2') %>%
gather() %>%
ggplot(., aes(x = value, fill = key)) +
geom_bar(position = 'dodge')+
ggtitle('Simple Random Sampling') +
theme(legend.position="none") +
xlab("response") +
scale_fill_brewer(palette = 'Set2')
# Plot of stratified
trainIndex <- createDataPartition(dat$y, times = 2, p = 0.75)
samples.caret <- cbind.data.frame(dat$y[trainIndex[[1]]],
dat$y[trainIndex[[2]]])
caret.plot <-
samples.caret %>%
gather() %>%
ggplot(., aes(x = value, fill = key)) +
geom_bar(position = 'dodge') +
ggtitle('Stratified Random Sampling')+
theme(legend.position="none") +
xlab("response")+
scale_fill_brewer(palette = 'Set2')
grid.arrange(samples.plot, caret.plot, ncol = 2)
preproc <- preProcess(train.x, c('center', 'scale'))
preproc
transformed <- predict(preproc, train.x) # This creates a new dataframe
data("Sacramento")
head(Sacramento)
lincombs <- findLinearCombos(model.matrix(price ~., data = Sacramento))
lincombs$remove # recommended columns to remove
data("GermanCredit")
idx <- createDataPartition(GermanCredit$Class, p = 0.75, list = FALSE)
train.GC <- GermanCredit[idx,]
test.GC <- GermanCredit[-idx,]
nearZeroVar(train.GC[ , -which(names(train.GC) %in% c("Class"))])
preProcess(train.GC[ , -which(names(train.GC) %in% c("Class"))], "nzv")
findLinearCombos(model.matrix(Class ~., data = GermanCredit))
# remove variables found from nearZeroVar
GermanCredit2 <-
GermanCredit[, -nearZeroVar(GermanCredit)]
# Remove variables found from findLinearCombos
GermanCredit.ready <-
GermanCredit2 %>%
select(-CheckingAccountStatus.lt.0, -SavingsAccountBonds.lt.100,
-EmploymentDuration.lt.1, -EmploymentDuration.Unemployed,
-Personal.Male.Married.Widowed, -Property.Unknown,
- Housing.ForFree)
# GermanCredit$CheckingAccountStatus.lt.0 <- NULL
# GermanCredit$SavingsAccountBonds.lt.100 <- NULL
# GermanCredit$EmploymentDuration.lt.1 <- NULL
# GermanCredit$EmploymentDuration.Unemployed <- NULL
# GermanCredit$Personal.Male.Married.Widowed <- NULL
# GermanCredit$Property.Unknown <- NULL
# GermanCredit$Housing.ForFree <- NULL
# fits model to training set without resampling
ctrl.none <- trainControl(method = 'none')
# 5-fold cross validation
ctrl.kcv <- trainControl(method = 'cv', number = 10)
#  10-pete of 10-fold cv
#ctrl.rpcv <- trainControl(method = 'repeatedcv', number = 10, repeats = 10)
# ctrl.oob <- trainControl(method = 'oob') # Out of bag sample
create.folds <- createFolds(train.dat$y, k = 10, returnTrain = TRUE)
str(create.folds)
gbm.fit <- train(y ~.,
data = train.dat,
# our cv method
trControl = ctrl.kcv,
method = 'gbm',
preProcess = c('center', 'scale'),
# This will keep gbm from printing a tome
verbose = FALSE,
metric = 'Accuracy')
gbm.fit <- train(y ~.,
data = train.dat,
# our cv method
trControl = ctrl.kcv,
method = 'gbm',
preProcess = c('center', 'scale'),
# This will keep gbm from printing a tome
verbose = FALSE,
metric = 'Accuracy')
summary(gbm.fit)
preds <- predict(gbm.fit, test.dat[, 1:5])
table(preds, test.dat$y)
confusionMatrix(preds, test.dat$y)
confusionMatrix(preds, as.factor(test.dat$y))
